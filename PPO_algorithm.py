import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List

# ============================================================================
# –ß–ê–°–¢–¨ 1: –ü–†–û–°–¢–ê–Ø –°–†–ï–î–ê CARTPOLE (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è —Ñ–∏–∑–∏–∫–∞)
# ============================================================================

class SimpleCartPole:
    """
    –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è CartPole –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ PPO.
    
    –§–∏–∑–∏–∫–∞:
    - –¢–µ–ª–µ–∂–∫–∞ –¥–≤–∏–∂–µ—Ç—Å—è –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –æ—Å–∏
    - –ù–∞ –Ω–µ–π –∑–∞–∫—Ä–µ–ø–ª—ë–Ω —à–µ—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å
    - –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–ª—É –≤–ª–µ–≤–æ (0) –∏–ª–∏ –≤–ø—Ä–∞–≤–æ (1)
    """
    
    def __init__(self):
        self.gravity = 9.8          # g - —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –ø–∞–¥–µ–Ω–∏—è
        self.cart_mass = 1.0        # –º–∞—Å—Å–∞ —Ç–µ–ª–µ–∂–∫–∏
        self.pole_mass = 0.1        # –º–∞—Å—Å–∞ —à–µ—Å—Ç–∞
        self.total_mass = self.cart_mass + self.pole_mass
        self.pole_length = 0.5      # –ø–æ–ª–æ–≤–∏–Ω–∞ –¥–ª–∏–Ω—ã —à–µ—Å—Ç–∞
        self.force_mag = 10.0       # —Å–∏–ª–∞ —Ç–æ–ª—á–∫–∞
        self.dt = 0.02              # —à–∞–≥ –≤—Ä–µ–º–µ–Ω–∏
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞
        self.x_threshold = 2.4      # —Ç–µ–ª–µ–∂–∫–∞ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ ¬±2.4
        self.theta_threshold = 12 * np.pi / 180  # —É–≥–æ–ª > 12 –≥—Ä–∞–¥—É—Å–æ–≤
        
        self.state = None
        self.steps = 0
        
    def reset(self) -> np.ndarray:
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        # –°–ª—É—á–∞–π–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–∏–∑–∫–æ –∫ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—é
        self.state = np.random.uniform(-0.05, 0.05, 4)
        self.steps = 0
        return self.state.copy()
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –≤ —Å—Ä–µ–¥–µ
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            state: –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ [x, x_dot, theta, theta_dot]
            reward: –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —à–∞–≥
            done: –∑–∞–≤–µ—Ä—à—ë–Ω –ª–∏ —ç–ø–∏–∑–æ–¥
        """
        x, x_dot, theta, theta_dot = self.state
        
        # –°–∏–ª–∞: -10 (–≤–ª–µ–≤–æ) –∏–ª–∏ +10 (–≤–ø—Ä–∞–≤–æ)
        force = self.force_mag if action == 1 else -self.force_mag
        
        # –§–∏–∑–∏–∫–∞: —É—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –º–∞—è—Ç–Ω–∏–∫–∞ –Ω–∞ —Ç–µ–ª–µ–∂–∫–µ
        cos_theta = np.cos(theta)
        sin_theta = np.sin(theta)
        
        # –ú–æ–º–µ–Ω—Ç –æ—Ç –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏—è —Ç–µ–ª–µ–∂–∫–∏
        temp = (force + self.pole_mass * self.pole_length * theta_dot**2 * sin_theta) / self.total_mass
        
        # –£–≥–ª–æ–≤–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ —à–µ—Å—Ç–∞: Œ∏Ãà
        theta_acc = (self.gravity * sin_theta - cos_theta * temp) / \
                    (self.pole_length * (4.0/3.0 - self.pole_mass * cos_theta**2 / self.total_mass))
        
        # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç–µ–ª–µ–∂–∫–∏: ·∫ç
        x_acc = temp - self.pole_mass * self.pole_length * theta_acc * cos_theta / self.total_mass
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º –≠–π–ª–µ—Ä–∞)
        x = x + self.dt * x_dot
        x_dot = x_dot + self.dt * x_acc
        theta = theta + self.dt * theta_dot
        theta_dot = theta_dot + self.dt * theta_acc
        
        self.state = np.array([x, x_dot, theta, theta_dot])
        self.steps += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        done = bool(
            x < -self.x_threshold or x > self.x_threshold or
            theta < -self.theta_threshold or theta > self.theta_threshold or
            self.steps >= 500
        )
        
        # –ù–∞–≥—Ä–∞–¥–∞: +1 –∑–∞ –∫–∞–∂–¥—ã–π —à–∞–≥, –ø–æ–∫–∞ —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–±–∏–ª—å–Ω–∞
        reward = 1.0
        
        return self.state.copy(), reward, done


# ============================================================================
# –ß–ê–°–¢–¨ 2: –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ –ù–ê –ß–ò–°–¢–û–ú NUMPY
# ============================================================================

def relu(x: np.ndarray) -> np.ndarray:
    """ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è: max(0, x)"""
    return np.maximum(0, x)

def relu_derivative(x: np.ndarray) -> np.ndarray:
    """–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è ReLU"""
    return (x > 0).astype(float)

def softmax(x: np.ndarray) -> np.ndarray:
    """
    Softmax –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    
    –§–æ—Ä–º—É–ª–∞: œÉ(x_i) = exp(x_i) / Œ£ exp(x_j)
    """
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


class ActorCriticNetwork:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å Actor-Critic
    
    Actor (–∞–∫—Ç—ë—Ä): –≤—ã–¥–∞—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π œÄ(a|s)
    Critic (–∫—Ä–∏—Ç–∏–∫): –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è V(s)
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
        Input (4) ‚Üí Hidden (64) ‚Üí Hidden (64) ‚Üí {Actor (2), Critic (1)}
    """
    
    def __init__(self, input_dim: int, hidden_dim: int, action_dim: int, lr: float = 3e-4):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ Xavier/He (—Å –º–µ–Ω—å—à–µ–π scale –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim) * 0.5
        self.b1 = np.zeros(hidden_dim)
        
        self.W2 = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim) * 0.5
        self.b2 = np.zeros(hidden_dim)
        
        # Actor head (–≥–æ–ª–æ–≤–∞ –∞–∫—Ç—ë—Ä–∞)
        self.W_actor = np.random.randn(hidden_dim, action_dim) * 0.01  # –ú–∞–ª–µ–Ω—å–∫–∏–µ –≤–µ—Å–∞ –¥–ª—è –ª–æ–≥–∏—Ç–æ–≤
        self.b_actor = np.zeros(action_dim)
        
        # Critic head (–≥–æ–ª–æ–≤–∞ –∫—Ä–∏—Ç–∏–∫–∞)
        self.W_critic = np.random.randn(hidden_dim, 1) * 0.01  # –ú–∞–ª–µ–Ω—å–∫–∏–µ –≤–µ—Å–∞ –¥–ª—è value
        self.b_critic = np.zeros(1)
        
        self.lr = lr
        
        # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (backprop)
        self.cache = {}
    
    def forward(self, state: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            action_probs: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π [œÄ(a=0|s), œÄ(a=1|s)]
            value: –æ—Ü–µ–Ω–∫–∞ V(s)
        """
        # –°–ª–æ–π 1
        z1 = state @ self.W1 + self.b1
        a1 = relu(z1)
        
        # –°–ª–æ–π 2
        z2 = a1 @ self.W2 + self.b2
        a2 = relu(z2)
        
        # Actor: –ª–æ–≥–∏—Ç—ã ‚Üí softmax ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        logits = a2 @ self.W_actor + self.b_actor
        action_probs = softmax(logits)
        
        # Critic: –ª–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥
        value = (a2 @ self.W_critic + self.b_critic)[0]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è backprop
        self.cache = {
            'state': state, 'z1': z1, 'a1': a1,
            'z2': z2, 'a2': a2, 'logits': logits,
            'action_probs': action_probs, 'value': value
        }
        
        return action_probs, value
    
    def backward(self, grad_logits: np.ndarray, grad_value: float):
        """
        –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (backpropagation) —Å gradient clipping
        
        grad_logits: –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ª–æ–≥–∏—Ç–∞–º –∞–∫—Ç—ë—Ä–∞
        grad_value: –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—ã—Ö–æ–¥—É –∫—Ä–∏—Ç–∏–∫–∞
        """
        state = self.cache['state']
        a1 = self.cache['a1']
        a2 = self.cache['a2']
        z1 = self.cache['z1']
        z2 = self.cache['z2']
        
        # === –ì–†–ê–î–ò–ï–ù–¢–´ –î–õ–Ø ACTOR HEAD ===
        grad_W_actor = a2.reshape(-1, 1) @ grad_logits.reshape(1, -1)
        grad_b_actor = grad_logits
        grad_a2_actor = grad_logits @ self.W_actor.T
        
        # === –ì–†–ê–î–ò–ï–ù–¢–´ –î–õ–Ø CRITIC HEAD ===
        grad_W_critic = a2.reshape(-1, 1) * grad_value
        grad_b_critic = np.array([grad_value])
        grad_a2_critic = self.W_critic.flatten() * grad_value
        
        # –°—É–º–º–∏—Ä—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç –æ–±–µ–∏—Ö –≥–æ–ª–æ–≤
        grad_a2 = grad_a2_actor + grad_a2_critic
        
        # === –°–ö–†–´–¢–´–ô –°–õ–û–ô 2 ===
        grad_z2 = grad_a2 * relu_derivative(z2)
        grad_W2 = a1.reshape(-1, 1) @ grad_z2.reshape(1, -1)
        grad_b2 = grad_z2
        grad_a1 = grad_z2 @ self.W2.T
        
        # === –°–ö–†–´–¢–´–ô –°–õ–û–ô 1 ===
        grad_z1 = grad_a1 * relu_derivative(z1)
        grad_W1 = state.reshape(-1, 1) @ grad_z1.reshape(1, -1)
        grad_b1 = grad_z1
        
        # === GRADIENT CLIPPING (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏!) ===
        max_grad_norm = 0.5
        
        grad_W1 = np.clip(grad_W1, -max_grad_norm, max_grad_norm)
        grad_b1 = np.clip(grad_b1, -max_grad_norm, max_grad_norm)
        grad_W2 = np.clip(grad_W2, -max_grad_norm, max_grad_norm)
        grad_b2 = np.clip(grad_b2, -max_grad_norm, max_grad_norm)
        grad_W_actor = np.clip(grad_W_actor, -max_grad_norm, max_grad_norm)
        grad_b_actor = np.clip(grad_b_actor, -max_grad_norm, max_grad_norm)
        grad_W_critic = np.clip(grad_W_critic, -max_grad_norm, max_grad_norm)
        grad_b_critic = np.clip(grad_b_critic, -max_grad_norm, max_grad_norm)
        
        # === –û–ë–ù–û–í–õ–ï–ù–ò–ï –í–ï–°–û–í (SGD) ===
        self.W1 -= self.lr * grad_W1
        self.b1 -= self.lr * grad_b1
        self.W2 -= self.lr * grad_W2
        self.b2 -= self.lr * grad_b2
        self.W_actor -= self.lr * grad_W_actor
        self.b_actor -= self.lr * grad_b_actor
        self.W_critic -= self.lr * grad_W_critic
        self.b_critic -= self.lr * grad_b_critic


# ============================================================================
# –ß–ê–°–¢–¨ 3: PPO –ê–õ–ì–û–†–ò–¢–ú
# ============================================================================

class PPOAgent:
    """
    Proximal Policy Optimization Agent
    
    –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
        epsilon: –ø–æ—Ä–æ–≥ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–±—ã—á–Ω–æ 0.2)
        gamma: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥
        lambda_gae: –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è GAE (Generalized Advantage Estimation)
        c1: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç value loss
        c2: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏
    """
    
    def __init__(self, state_dim: int, action_dim: int, 
                 epsilon: float = 0.2, gamma: float = 0.99, 
                 lambda_gae: float = 0.95, c1: float = 0.5, c2: float = 0.01):
        self.network = ActorCriticNetwork(state_dim, 64, action_dim, lr=1e-4)  # –£–º–µ–Ω—å—à–∏–ª–∏ LR!
        
        self.epsilon = epsilon      # Œµ –¥–ª—è –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏—è
        self.gamma = gamma          # Œ≥ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.lambda_gae = lambda_gae  # Œª –¥–ª—è GAE
        self.c1 = c1                # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç value loss
        self.c2 = c2                # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏
        
    def select_action(self, state: np.ndarray) -> Tuple[int, float, float]:
        """
        –í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–µ
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            action: –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            log_prob: –ª–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è
            value: –æ—Ü–µ–Ω–∫–∞ V(s)
        """
        action_probs, value = self.network.forward(state)
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç NaN
        if np.isnan(action_probs).any() or np.isnan(value):
            print("‚ö†Ô∏è  Warning: NaN detected! Using random policy.")
            action_probs = np.ones(len(action_probs)) / len(action_probs)
            value = 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        action_probs = action_probs / (action_probs.sum() + 1e-8)
        
        # –°—ç–º–ø–ª–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        action = np.random.choice(len(action_probs), p=action_probs)
        
        # log œÄ(a|s) - –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è ratio
        log_prob = np.log(action_probs[action] + 1e-8)
        
        return action, log_prob, value
    
    def compute_gae(self, rewards: List[float], values: List[float], 
                    dones: List[bool]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generalized Advantage Estimation (GAE)
        
        –§–æ—Ä–º—É–ª–∞:
            Œ¥_t = r_t + Œ≥¬∑V(s_{t+1}) - V(s_t)  (TD error)
            A_t = Œ£ (Œ≥Œª)^k ¬∑ Œ¥_{t+k}
        
        GAE –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É:
        - –ù–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π (Œª‚Üí0): A = Œ¥ (1-step TD)
        - –ù–∏–∑–∫–∏–º —Å–º–µ—â–µ–Ω–∏–µ–º (Œª‚Üí1): A = Œ£ Œ≥^k¬∑r (Monte Carlo)
        """
        advantages = []
        returns = []
        
        gae = 0
        next_value = 0
        
        # –ò–¥—ë–º —Å –∫–æ–Ω—Ü–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        for t in reversed(range(len(rewards))):
            if dones[t]:
                next_value = 0
                gae = 0
            
            # TD error: Œ¥ = r + Œ≥¬∑V(s') - V(s)
            delta = rewards[t] + self.gamma * next_value - values[t]
            
            # GAE: A = Œ¥ + (Œ≥Œª)¬∑A_{t+1}
            gae = delta + self.gamma * self.lambda_gae * gae
            
            advantages.insert(0, gae)
            
            # Return: G = A + V (advantage + baseline)
            returns.insert(0, gae + values[t])
            
            next_value = values[t]
        
        advantages = np.array(advantages)
        returns = np.array(returns)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è advantages (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è returns –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ critic (–í–ê–ñ–ù–û!)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return advantages, returns
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_log_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, epochs: int = 10):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é PPO
        
        –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        """
        n_samples = len(states)
        
        for epoch in range(epochs):
            # Shuffling –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            indices = np.random.permutation(n_samples)
            
            for idx in indices:
                state = states[idx]
                action = actions[idx]
                old_log_prob = old_log_probs[idx]
                advantage = advantages[idx]
                return_target = returns[idx]
                
                # === FORWARD PASS ===
                action_probs, value = self.network.forward(state)
                
                # –õ–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
                new_log_prob = np.log(action_probs[action] + 1e-8)
                
                # === PPO CLIPPED OBJECTIVE ===
                # Ratio: r = œÄ_new / œÄ_old = exp(log œÄ_new - log œÄ_old)
                ratio = np.exp(new_log_prob - old_log_prob)
                
                # –î–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ objective:
                # 1) r¬∑A
                surr1 = ratio * advantage
                
                # 2) clip(r, 1-Œµ, 1+Œµ)¬∑A
                surr2 = np.clip(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage
                
                # –ë–µ—Ä—ë–º –º–∏–Ω–∏–º—É–º (–ø–µ—Å—Å–∏–º–∏—Å—Ç–∏—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
                actor_loss = -np.minimum(surr1, surr2)
                
                # === VALUE FUNCTION LOSS (—Å –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏) ===
                value_error = return_target - value
                value_error = np.clip(value_error, -10, 10)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—à–∏–±–∫—É!
                value_loss = self.c1 * value_error ** 2
                
                # === ENTROPY BONUS (–¥–ª—è exploration) ===
                # H(œÄ) = -Œ£ œÄ(a|s)¬∑log œÄ(a|s)
                entropy = -np.sum(action_probs * np.log(action_probs + 1e-8))
                entropy_loss = -self.c2 * entropy
                
                # === TOTAL LOSS ===
                total_loss = actor_loss + value_loss + entropy_loss
                
                # === BACKWARD PASS ===
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ª–æ–≥–∏—Ç–∞–º –∞–∫—Ç—ë—Ä–∞
                grad_logits = action_probs.copy()
                
                # –î–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è:
                if advantage > 0:
                    # –•–æ—Ä–æ—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
                    if ratio > 1 + self.epsilon:
                        grad_logits[action] = 0  # –ù–µ –æ–±–Ω–æ–≤–ª—è–µ–º, –µ—Å–ª–∏ —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ–ª–∏—á–∏–ª–∏
                    else:
                        grad_logits[action] -= 1 / (action_probs[action] + 1e-8) * advantage
                else:
                    # –ü–ª–æ—Ö–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: —É–º–µ–Ω—å—à–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                    if ratio < 1 - self.epsilon:
                        grad_logits[action] = 0
                    else:
                        grad_logits[action] -= 1 / (action_probs[action] + 1e-8) * advantage
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏ (–¥–ª—è exploration)
                grad_logits += self.c2 * (np.log(action_probs + 1e-8) + 1)
                
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç –∫—Ä–∏—Ç–∏–∫–∞ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
                grad_value = -2 * self.c1 * value_error
                grad_value = np.clip(grad_value, -1.0, 1.0)  # –ö–ª–∏–ø–ø–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç!
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
                self.network.backward(grad_logits, grad_value)


# ============================================================================
# –ß–ê–°–¢–¨ 4: –û–ë–£–ß–ï–ù–ò–ï
# ============================================================================

def train_ppo(episodes: int = 500, max_steps: int = 500):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è PPO –Ω–∞ CartPole"""
    env = SimpleCartPole()
    agent = PPOAgent(state_dim=4, action_dim=2)
    
    episode_rewards = []
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ PPO –Ω–∞ CartPole!")
    print("=" * 60)
    
    for episode in range(episodes):
        state = env.reset()
        
        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        states, actions, rewards = [], [], []
        old_log_probs, values, dones = [], [], []
        
        episode_reward = 0
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
        for step in range(max_steps):
            action, log_prob, value = agent.select_action(state)
            next_state, reward, done = env.step(action)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            old_log_probs.append(log_prob)
            values.append(value)
            dones.append(done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # –í—ã—á–∏—Å–ª—è–µ–º advantages –∏ returns
        advantages, returns = agent.compute_gae(rewards, values, dones)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É (–º–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
        agent.update(
            np.array(states),
            np.array(actions),
            np.array(old_log_probs),
            advantages,
            returns,
            epochs=5  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 10 –¥–æ 5
        )
        
        episode_rewards.append(episode_reward)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if (episode + 1) % 20 == 0:
            avg_reward = np.mean(episode_rewards[-20:])
            print(f"Episode {episode + 1:4d} | Reward: {episode_reward:6.1f} | "
                  f"Avg(20): {avg_reward:6.1f}")
    
    return episode_rewards


# ============================================================================
# –ó–ê–ü–£–°–ö
# ============================================================================

if __name__ == "__main__":
    rewards = train_ppo(episodes=30000)
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(rewards, alpha=0.3, label='Raw rewards')
    plt.plot(np.convolve(rewards, np.ones(20)/20, mode='valid'), 
             label='Moving average (20)', linewidth=2)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('PPO Learning Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(np.convolve(rewards, np.ones(20)/20, mode='valid'), linewidth=2)
    plt.xlabel('Episode')
    plt.ylabel('Average Reward (20 episodes)')
    plt.title('Smoothed Performance')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\n" + "=" * 60)
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(rewards[-50:]):.1f}")
    print("=" * 60)
